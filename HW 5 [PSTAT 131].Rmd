---
title: "Homework 5 [PSTAT 131]"
author: "Charlotte Sielewicz"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

Load libraries:

```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
tidymodels_prefer()

```

Load the Pokemon data:

```{r}
Pokemon <- read.csv("/Users/charlottesielewicz/Documents/UCSB/2022-Spring Quarter/PSTAT 131/homework-5/data/Pokemon.csv")
```

### Exercise 1

```{r}
library(janitor)
pokemon <- Pokemon %>%
  clean_names()
```

The variables in the data changed title to become more consistent. For example `Type.1` became `type_1` and `Sp..Def` became `sp_def`. This can be useful when referencing variables later on. It can get very confusing if a lot of time is spent double checking variable names. Additionally if there is no consistency there is the danger of writing the variable wrong in code and wasting time looking for an error that could have easily been avoided if there was consistency.

### Exercise 2

Bar chart of `type_1` with entire data set:

```{r}
pokemon %>%
ggplot(aes(x = type_1)) +
geom_bar()
```

There are 18 classes with Pokemon type 'Flying' having only a few Pokemon.

```{r}
#filter the pokemon dataset
pokemon <- pokemon%>% 
  filter(type_1 =='Bug' | type_1 == 'Fire' | type_1 == 'Grass' | type_1 == 'Normal' | type_1 == 'Water' | type_1 == 'Psychic') 

# Factor variables type_1 and legendary
pokemon$type_1 <- factor(pokemon$type_1)
pokemon$legendary <- factor(pokemon$legendary)
pokemon$generation <- factor(pokemon$generation)

pokemon %>%
ggplot(aes(x = type_1)) +
geom_bar()
```

### Exercise 3

Create a recipe:

```{r}
set.seed(3435)

pokemon_split <- initial_split(pokemon, prop = 0.80,
                                strata = type_1) #stratify `survived`
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
```

`pokemon_train` has 438(0.70) = 350.4 $\approx$ 318 observations

`pokemon_test` has 438(0.20) = 87.6 $\approx$ 94 observations

Why might stratifying the folds be useful?

```{r}
pokemon_fold <- vfold_cv(pokemon_train, v = 5)
```

### Exercise 4

set of recipe:

```{r}
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pokemon_train) %>%
  step_dummy(legendary) %>% 
  step_dummy(generation) %>%
  step_normalize(all_predictors()) # center and sclae all predictors
```

### Exercise 5

Five models will be fit.

```{r}
pokemon_spec <- multinom_reg(mode = "classification",
  engine = "glmnet",
  penalty = tune(),
  mixture = tune())

pokemon_wkflow <- workflow() %>% 
  add_recipe(pokemon_recipe) %>% 
  add_model(pokemon_spec)

pokemon_grid <- grid_regular(penalty(range = c(-5, 5)),mixture(range = c(0,1)), levels = 10)
pokemon_grid
```

### Exercise 6

Fit the models to your folded data using `tune_grid()`.

Use `autoplot()` on the results. What do you notice? Do larger or smaller values of `penalty` and `mixture` produce better accuracy and ROC AUC?

```{r}
tune_res <- tune_grid(
  pokemon_wkflow,
  resamples = pokemon_fold, 
  grid = pokemon_grid
)

autoplot(tune_res)
```

### Exercise 7

Use `select_best()` to choose the model that has the optimal `roc_auc`. Then use `finalize_workflow()`, `fit()`, and `augment()` to fit the model to the training set and evaluate its performance on the testing set.

```{r}
best_penalty <- select_best(tune_res, metric = "roc_auc")

pokemon_final <- finalize_workflow(pokemon_wkflow, best_penalty)

pokemon_final_fit <- fit(pokemon_final, data = pokemon_train)

augment(pokemon_final_fit, new_data = pokemon_test) %>%
  roc_curve(truth = type_1, estimate = c(.pred_Bug,.pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water))
```

### Exercise 8

ROC AUC:

```{r}
augment(pokemon_final_fit, new_data = pokemon_test) %>%
roc_auc(truth = type_1, estimate = c(.pred_Bug,.pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water))

```

ROC Curve:

```{r}
augment(pokemon_final_fit, new_data = pokemon_test) %>%
  roc_curve(truth = type_1, estimate = c(.pred_Bug,.pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water)) %>%
  autoplot()
```

Heat map of confusion matrix.

```{r}
augment(pokemon_final_fit, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

The model only did well for some variables categories, `Normal` and `Bug`, while it did not predict some of the other variables well, `Grass` and `Fire`. This could be because strength varies between different categories in a way that does not follow an obvious pattern according to the computer.
