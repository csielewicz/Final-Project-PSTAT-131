---
title: "Homework 3 [PSTAT 131]"
author: "Charlotte Sielewicz"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
library(tidymodels)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) 
tidymodels_prefer()
```

Load the data from `tatanic.csv` onto *R*:

```{r}
titanic <- read.csv("/Users/charlottesielewicz/Documents/UCSB/2022-Spring Quarter/PSTAT 131/homework-3/data/titanic.csv")
```

Change `survived` and `pclass` into factor variables such that "Yes" is the first level for the `survived` variable:

```{r}
titanic$survived <- factor(titanic$survived, levels = c("Yes", "No"))

titanic$pclass <- factor(titanic$pclass)
```

Set a seed:

```{r}
set.seed(3435)
```

### Question 1

Split the data, creating training and testing sets:

```{r}
titanic_split <- initial_split(titanic, prop = 0.70,
                                strata = survived) #stratify `survived`
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

```

`titanic_train` has 891(0.70) $\approx$ 623 observations in the training dataset there is missing data in both the `age` variable and the `cabin` variable.

`titanic_test` has 891(0.30) $\approx$ 268 observations

Using stratified data for this dataset allows for the training and testing sets to have a better range of data that creates a more accurate prediction. Because the likelihood of surviving for the passengers was based on age, class,...etc. stratifying the data takes all of these influences into account.

### Question 2

Explore the outcome of `survived` in the training set:

```{r}
titanic_train %>% 
  ggplot(aes(x = survived)) +
  geom_bar()
```

The training set shows much over half of the passengers did not survive.

### Question 3

Create a correlation matrix:

```{r}
Titanic_train <- titanic_train %>%
  select(is.numeric)

cor_titanic_train <- Titanic_train %>%
  correlate()

cor_titanic <- Titanic_train %>%
  correlate()
rplot(cor_titanic)
```

`age` (ticket price) and `sib_sp` (number of siblings or spouses on board) are negatively correlated with each other. And, `sib_sp` and `parch` (number of parents or children on board) are positively correlated with each other. `parch` and `fare` are also positively correlated but not strongly. Same with `age` and `parch` but negative.

### Question 4

Create a training set including the predictors `pclass`, `sex`, `age`, `sib_sp`, `parch` and `fare`:

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>%
  step_impute_linear(age) %>% # Deal with missing values in `age` variable
  step_dummy(all_nominal_predictors()) %>% # ecode categorical predictors
  step_interact(~sex:fare) %>% # interaction between sex and passenger fare
  step_interact(~age:fare) # interaction between age and passenger fare
```

### Question 5

Specify a **logistic regression** model, create a workflow and apply workflow to training data:

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

log_fit <- fit(log_wkflow, titanic_train)

```

### Question 6

Specify a **LDA** model, create a workflow and apply workflow to training data:

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)
```

### Question 7

Specify a **QDA** model, create a workflow and apply workflow to training data:

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, titanic_train)
```

### Question 8

Specify a **Naive Bayes** model, create a workflow and apply workflow to training data:

```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, titanic_train)
```

### Question 9

Fit the models to the training data and find which model achieved the highest accuracy on the training data.

#### Logistic Regression:

```{r}
predict(log_fit, new_data = titanic_train)

augment(log_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class)

log_acc <- augment(log_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
```

#### LDA:

```{r}
predict(lda_fit, new_data = titanic_train)

augment(lda_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class)

lda_acc <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
```

#### QDA:

```{r}
predict(qda_fit, new_data = titanic_train)

augment(qda_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class)

qda_acc <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
```

#### Naive Bayes:

```{r}
predict(nb_fit, new_data = titanic_train)

augment(nb_fit, new_data = titanic_train) %>%
  conf_mat(truth = survived, estimate = .pred_class)

nb_acc <- augment(nb_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
```

#### Accuracies

```{r}
accuracies <- c(log_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```

### Question 10

#### Logistic model fit to testing data:

```{r}
predict(log_fit, new_data = titanic_test)
log_acc_test <- augment(log_fit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)
accuracies_test <- log_acc_test$.estimate
models_test <- "Logistic Regression - Testing data"
results <- tibble(accuracies = accuracies_test, models = models_test)
results %>% 
  arrange(-accuracies_test)
```

#### Confusion matrix:

```{r}
augment(log_fit, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) 
```

#### ROC curve:

```{r}
augment(log_fit, new_data = titanic_test) %>%
  roc_curve(survived, .pred_Yes) %>%
  autoplot()
```

#### AUC:

```{r}
augment(log_fit, new_data = titanic_test) %>%
  roc_auc(survived, .pred_Yes)
```

The logistic regression model performed relatively well it is curved in such a way that indicates a good prediction.

```{r}
accuracies_compare <- c(log_acc$.estimate, log_acc_test$.estimate)
models_compare <- c("Training Accuracies", "Testing Accuracies")
results_compare <- tibble(accuracies = accuracies_compare, models = models_compare)
results_compare %>% 
  arrange(-accuracies_compare)
```

The testing and training accuracies differ by 0.0236219. This could be impacted by some unexpected/out of the ordinary occurrences. The sinking of the titanic was a real event, and thus, some people survived against all odds while others died even when they had a higher chance of surviving.
