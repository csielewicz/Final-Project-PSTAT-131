---
title: "Homework 4 [PSTAT 131]"
author: "Charlotte Sielewicz"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
library(tidymodels)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) 
tidymodels_prefer()
```

Load the `titanic` data:

```{r}
titanic <- read.csv("/Users/charlottesielewicz/Documents/UCSB/2022-Spring Quarter/PSTAT 131/homework-4/data/titanic.csv")
```

Factor the variables `survived` and `pclass`:

```{r}
titanic$survived <- factor(titanic$survived, levels = c("Yes", "No"))

titanic$pclass <- factor(titanic$pclass)
```

Set a seed:

```{r}
set.seed(3435)
```

### Question 1

Split the data, stratifying on the outcome variable, `survived.` You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations.

```{r}
titanic_split <- initial_split(titanic, prop = 0.70,
                                strata = survived) #stratify `survived`
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```

`titanic_train` has 891(0.70) $\approx$ 623 observations in the training dataset there is missing data in both the `age` variable and the `cabin` variable.

`titanic_test` has 891(0.30) $\approx$ 268 observations

Using stratified data for this dataset allows for the training and testing sets to have a better range of data that creates a more accurate prediction. Because the likelihood of surviving for the passengers was based on age, class,...etc. stratifying the data takes all of these influences into account.

### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```

### Question 3

In question 2 we are splitting the training data into 10 random groups. It then creates new training and testing data within the smaller groups. This allows us to fit models to each of the 10 subgroups. K-fold cross-validation takes out the first group (fold) to find the MSE for the rest of the groups/fold. This is then repeated so that each fold is used as a validation set. The CV then becomes the average of each of the MSEs found. This allows us to find a much more consistent model. If we were to use the entire training set instead, we would use the validation set approach.

### Question 4

Set up workflows for 3 models:

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>%
  step_impute_linear(age) %>% # Deal with missing values in `age` variable
  step_dummy(all_nominal_predictors()) %>% # ecode categorical predictors
  step_interact(terms = ~ starts_with("sex"):fare) %>% # interaction between sex and passenger fare
  step_interact(~age:fare) %>% # interaction between age and passenger fare
  step_poly(degree = tune())

```

1.  A logistic regression with the `glm` engine;

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)
```

1.  A linear discriminant analysis with the `MASS` engine;

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)
```

1.  A quadratic discriminant analysis with the `MASS` engine.

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```

We set the k-fold cross-validation to do 10 folds, thus there will be 30 models total fitting to the data.

### Question 5

```{r}
degree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)

log_tune <- tune_grid(
  object = log_wkflow, 
  resamples = titanic_folds,
  grid = degree_grid,
  control = control_grid(verbose = TRUE)
)

lda_tune <- tune_grid(
  object = lda_wkflow, 
  resamples = titanic_folds,
  grid = degree_grid,
  control = control_grid(verbose = TRUE)
)

qda_tune <- tune_grid(
  object = qda_wkflow, 
  resamples = titanic_folds,
  grid = degree_grid,
  control = control_grid(verbose = TRUE)
)

```

### Question 6

```{r}
log_metrics <- collect_metrics(log_tune)
log_average = tibble(sum(log_metrics$mean)/20, sum(log_metrics$std_err)/20)
log_average
show_best(log_tune, metric = "accuracy")
```

```{r}
lda_metrics <- collect_metrics(lda_tune)

lda_average = tibble(sum(lda_metrics$mean)/20, sum(lda_metrics$std_err)/20)
lda_average

show_best(lda_tune, metric = "accuracy")
```

```{r}
qda_metrics <- collect_metrics(qda_tune)
qda_average = tibble(sum(qda_metrics$mean)/20, sum(qda_metrics$std_err)/20)
qda_average

show_best(qda_tune, metric = "accuracy")
```

Taking the mean and standard error of the QDA model we can see that the accuracy of the logistic regression does not fall within the standard error of the QDA model. Thus, the logistic regression model has performed better.

Now comparing the logistic regression model to the LDA model we can see that the mean for the logistic regression model is much higher while the standard errors are very similar. Thus, the logistic regression model has performed the best.

### Question 7

##### Fit logistic regression model:

```{r}
best_degree <- select_by_one_std_err(log_tune, degree, metric = "accuracy")
final_wf <- finalize_workflow(log_wkflow, best_degree)

final_fit <- fit(final_wf, titanic_train)
```

### Question 8

Comparing predictions:

```{r}
titanic_predict <- predict(final_fit, new_data = titanic_test %>% select(-survived))

titanic_predict <- bind_cols(titanic_predict, titanic_test %>% select(survived))
titanic_predict %>% 
  head()

```

Comparing accuracy:

```{r}
log_acc <- augment(final_fit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)

log_acc <- bind_cols(log_acc, new_data = log_average)
log_acc
```

The model's testing is higher than the average testing accuracy across the training data.
